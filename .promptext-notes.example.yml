# Promptext-Notes Configuration File
# Copy this file to .promptext-notes.yml and customize for your project

version: "1"

# AI Provider Configuration
ai:
  # Provider to use: anthropic, openai, cerebras, groq, openrouter, ollama
  provider: cerebras

  # Model to use (exact API model name - no aliases)
  # Use the exact model name from the provider's API documentation
  # Anthropic: claude-haiku-4-5, claude-sonnet-4-5, claude-opus-4-20250514
  # OpenAI: gpt-4o-mini, gpt-4o, gpt-4-turbo
  # Cerebras: zai-glm-4.6, gpt-oss-120b, llama-3.3-70b
  # Groq: llama-3.3-70b-versatile, mixtral-8x7b-32768, moonshotai/kimi-k2-instruct-0905
  # OpenRouter: openai/gpt-4o-mini, anthropic/claude-sonnet-4, google/gemini-pro-1.5, etc.
  #   (format: provider/model - access hundreds of models through one API)
  # Ollama: llama3.2, codellama, etc. (models you have pulled locally)
  model: zai-glm-4.6

  # Environment variable containing API key
  # Set to empty string for Ollama (no API key needed)
  api_key_env: CEREBRAS_API_KEY

  # Maximum tokens for AI response
  max_tokens: 8000

  # Temperature for AI generation (0.0 to 1.0)
  # Lower = more focused, Higher = more creative
  temperature: 0.3

  # Timeout for AI requests
  timeout: 30s

  # Retry configuration
  retry:
    # Number of retry attempts
    attempts: 3

    # Backoff strategy: exponential, linear, constant
    backoff: exponential

    # Initial delay before first retry
    initial_delay: 2s

  # Provider-specific custom options
  custom:
    # Anthropic API version (optional)
    # anthropic_version: "2023-06-01"

    # OpenRouter optional headers for rankings on openrouter.ai
    # http_referer: "https://your-app-url.com"  # Your app's site URL
    # x_title: "Your App Name"                  # Your app's title

    # Ollama base URL (optional, defaults to http://localhost:11434)
    # ollama_url: "http://localhost:11434"

  # 2-Stage Polish Workflow (optional)
  # Stage 1 (Discovery): Uses the main ai.provider and ai.model above
  # Stage 2 (Polish): Uses the polish_model and polish_provider below
  polish:
    # Enable 2-stage workflow (can also use --polish CLI flag)
    enabled: false

    # Polish model (stage 2) - defaults to main ai.model if not specified
    # Use a model optimized for polished, customer-facing content
    # Examples: "google/gemini-2.5-flash", "anthropic/claude-sonnet-4"
    polish_model: ""

    # Optional: Different provider for polish stage (defaults to main ai.provider)
    # Example: Use Cerebras for discovery (main config), OpenRouter for polish
    # polish_provider: "openrouter"

    # Optional: API key env var for polish provider (auto-detected if not specified)
    # polish_api_key_env: "OPENROUTER_API_KEY"

    # Optional: Custom polish prompt (uses default if not specified)
    # The placeholder %s will be replaced with the draft changelog
    # polish_prompt: ""

    # Polish stage settings
    polish_max_tokens: 4000
    polish_temperature: 0.3

    # Example configuration for recommended 2-stage workflow:
    # Main config uses Cerebras llama-3.3-70b for discovery (FREE)
    # enabled: true
    # polish_model: "google/gemini-2.5-flash"   # $0.0074/run on OpenRouter
    # polish_provider: "openrouter"
    # polish_api_key_env: "OPENROUTER_API_KEY"

# Output Configuration
output:
  # Output format: keepachangelog, conventional
  format: keepachangelog

  # Sections to include in changelog
  sections:
    - breaking
    - added
    - changed
    - fixed
    - docs

  # Custom template path (optional)
  # template: ./templates/custom-changelog.tmpl

# Filtering Configuration
filters:
  # File filters
  files:
    # Include patterns (glob format)
    include:
      - "*.go"
      - "*.md"
      - "*.yml"
      - "*.yaml"
      - "*.json"
      - "*.js"
      - "*.ts"
      - "*.tsx"
      - "*.py"

    # Exclude patterns (by filename - useful for preventing meta-documentation
    # from influencing AI-generated changelogs)
    # NOTE: Add files here that you don't want the AI to see when generating changelogs
    exclude:
      - "CHANGELOG.md"               # Prevent AI from seeing existing changelog
      - "README.md"                  # Exclude general project documentation
      - ".promptext-notes.example.yml"  # Exclude example config
      - "*_test.go"                  # Exclude test files
      - "vendor/*"                   # Exclude vendor dependencies
      - "node_modules/*"             # Exclude Node.js dependencies
      - ".git/*"                     # Exclude git metadata
      - "dist/*"                     # Exclude build output
      - "build/*"                    # Exclude build artifacts

  # Commit filters
  commits:
    # Exclude commits from these authors
    exclude_authors:
      - "dependabot[bot]"
      - "renovate[bot]"
      - "github-actions[bot]"

    # Exclude commits matching these patterns (regex)
    exclude_patterns:
      - "^Merge pull request"
      - "^Merge branch"
      - "^chore\\(deps\\):"

# Usage Examples:
#
# 1. Generate AI-enhanced changelog (uses config file):
#    promptext-notes --generate
#
# 2. Override provider via CLI:
#    promptext-notes --generate --provider openai --model gpt-4o
#
# 3. Generate for specific version range:
#    promptext-notes --generate --version v1.0.0 --since v0.9.0
#
# 4. Use OpenRouter (access hundreds of models through one API):
#    promptext-notes --generate --provider openrouter --model anthropic/claude-sonnet-4
#
# 5. Use local Ollama (free, no API key):
#    promptext-notes --generate --provider ollama --model llama3.2
#
# 6. Save to file:
#    promptext-notes --generate --output CHANGELOG.md
